<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>TEAM 35: Project Midterm</title>
<link rel="stylesheet" href="styles.css">
<link rel="shortcut icon" href="img/Logo1.png" >
</head>
<header>
<h1 style="color: mediumaquamarine;">Team 35 <text style="color: white"> Project Midterm</h1> 
<CS style="color: mediumaquamarine"><em>CS 7641</em></CS> Team Members: Rachit Chadha, Khalid Shaikh, Mini Jain, Asmit Kumar Singh, Amrit Khera
</header>
<style> 
    button { 
background-color: #032833;
  border: none;
  color: white;
  padding: 15px 32px;
  text-align: center;
  text-decoration:rgba(255, 255, 255, 0.1);
  display: inline-block;
  font-size: 16px; 
    } 
  </style> 
<section id="introduction">
<h2>Introduction/Background</h2>
<p>We have all experienced that curiosity that arises while watching anime, pondering the meaning of the text displayed within the episodes. Animes frequently employ textual visuals to introduce new storylines or conclude existing ones, and these elements play a pivotal role in the overall viewing experience. Therefore, it would be immensely beneficial if this text could be seamlessly translated and integrated into the video, preserving its original style, including the color and font, while altering the content itself.</p>
<p>The potential applications of solving this challenge extend beyond anime, to translating movie or show posters effortlessly into the desired language, making real-time text translations accessible and natural.</p>
<br><br><img src="img/intro.png" alt="Problem Sample Intro" height="300" width="800" style="padding-left: 15%;">
<br><br> <button style="margin-left: 30%;"><a href="https://github.com/rchadha33/Group-35">Here is a Link To Our Github Repository</a></button>
</section>
<section id="problem-definition">
<h2>Problem Definition</h2>
<p>The objective of this project is to transform a Japanese image, including posters, anime frames, or real-world images, into English while preserving the essence, meaning, and visual style of the original image. This intricate challenge can be deconstructed into a pipeline as shown below.</p>
<img src="img/pd.png" alt="Problem pipeline" height="300" width="800" style="padding-left: 12%;">
<p>The OCR and translation models have been the center of many research problems, and hence it's a solved problem, with no significant scope for improvement. The text eraser model detects and removes any textual elements within the image and fills the void seamlessly, maintaining the image's integrity, and is efficiently performed by models like DALLE. Hence, the primary focus of our project would be the Inter-Language font transfer model.</p>
</section>
<section id="Dataset">
<h2>Dataset</h2>
<img src="img/dataset.png" alt="Figure 1. Dataset Creation" height="350" width="850" style="padding-left: 12%;">
<p>The methodologies that we are using require 2 sets of images, the input and the target. The image translation models will learn the given Japanese text style and apply it to the translated English characters to get English text in the original style. Hence the model needs information on both the content and style of the image. The figure shows the process of our dataset creation with font .ttf files as our starting point, sourced from font websites like https://www.freejapanesefont.com, to generate sample input and output pairs. The input image tries to capture the style using Japanese font characters, while the input English character is passed in the middle. We have created a dataset of 10,000 samples.</p>

<h4 style="color: teal;">DATASET CREATION</h4>
<p>The dataset was created by using Japanese font characters in our training images juxtaposed with the English character in standard font, and the corresponding pair of that training image was the English character stylized in the Japanese font. We created two sets of data, one in which we used 8 Japanese characters for each English character, and another with 3 Japanese stylized characters for each English character. This allowed our model to learn diversified input.  Both sets of data can be seen in Fig 4 below. We ran our experiments for both sets of data to determine which training input helped the model generate the stylized English character better.
</p>
<img src="img/create.png" alt="Dataset Creation" height="300" width="800" style="padding-left: 12%;">
<h4 style="color: teal;">DATASET PREPROCESSING</h4>
<p>The preprocessing of the dataset plays a pivotal role in enhancing the robustness and generalization capabilities of the model. Hence, we applied varied augmentations to our original dataset to enhance our dataset by normalizing, rotating, and adding random cropping and jitter to the images in the dataset. Using these preprocessing techniques allows the model to handle a wide array of variations in orientations, and image quality of input images. It also enhances the model's resilience to different writing styles and helps prevent overfitting to specific patterns present in the training data.
</p>
<p>Further, we combined the two images in each data sample, to create a single input image for the model. The pairing of images allowed for unsupervised learning of the GAN, with each pair consisting of the Japanese stylization as well as English stylization, as seen in Fig 5.
</p>
<img src="img/visualization/Combined_AB.png" alt="Dataset Creation" height="300" width="800" style="padding-left: 12%;">
<br><br> <button style="margin-left: 30%;"><a href="https://drive.google.com/drive/folders/15GHkmcLEM4HfFKeUVJAhxhTU5eUbNoAI?usp=sharing">Link to our Dataset</a></button>
</section>

<section id="methods">
<h2>Methods</h2>
<p><h3 style="text-align:center";><u><b>1.</b>Pix2Pix / Pix2PixHD</u></h3></p><br>
<img src="img/p2p1.png" alt="Figure 2. Pix2Pix image translation example" height="300" width="800" style="padding-left: 12%;">
<p>Pix2pix is an image-to-image translation model, that translates the style (scene) of an image to another representation [1].</p>
<p>It's based on a Conditional Generative Adversarial Network (cGAN) architecture where a generator and discriminator are trained simultaneously in a min-max fashion. Pix2PixHD is an extension of Pix2Pix for higher resolution images using a multiscale generator [2], trained as shown in Fig 3.</p><br>
<img src="img/p2p2.png" alt="Figure 3. Pix2Pix Training procedure. (a) Generator Training (b) Discriminator Training" height="500" width="1000" style="padding-left: 10%;">
<br><br>
<p><h3 style="text-align:center";><u><b>2. </b>CycleGAN</u></h3></p>
<p>CycleGAN is also a cGAN like Pix2Pix designed for unpaired image-image translation tasks when paired training data is unavailable [3]. </p><br>
<img src="img/cg1.png" alt="Figure 4. CycleGAN image translation example" height="300" width="900" style="padding-left: 10%;">
<p>CycleGAN follows a similar training pattern as Pix2Pix with 2 generators and discriminators in a cyclic manner instead of a single one:</p>
<img src="img/cg2.png" alt="Figure 5. CycleGAN training where G,F are generators and Dx and Dy are discriminators" height="300" width="800" style="padding-left: 18%;">
</section>
<section id="Proposed Methodology">
<h2>Proposed Methodology</h2>
<p>Our methodology uses these state-of-the-art cGAN models (Pix2Pix, Cycle GAN) to transfer the <em>style</em> of Japanese characters in visual media to English. </p><br>
<img src="img/pm1.png" alt="Figure 6. Proposed Methodology" height="450" width="1000" style="padding-left: 10%;">
<p><h3>Steps:</h3></p>
<ol>
    <li>Extract and translate Japanese text from the input image to English.</li>
    <li>Use our dataset to train the style transfer between Japanese and English characters.</li>
    <li>Apply the trained model to translate the style of the English characters.</li>
    <li>Overlay the stylized English text back on the image.</li>
</ol>

</section>

<section id="Results and Discussion">
  <h2>Results and Discussion</h2>
    <p>Till now, we have experimented with the Pix2Pix models for 2 different dataset configurations. We tried the 3jap+1eng and 8jap+1eng configurations. The 3jap+1eng results were significantly bad and the model did not seem to understand the difference between the target character and the style japanese characters. Therefore we went ahead with the 8jap+1eng model which we trained over 100 epochs and 10,000 training samples.</p>
  <img src="img/visualization/Epoch 100 Sample.png" alt="Dataset Creation" height="500" width="900" style="padding-left: 12%;">

  <p>We can see that the pix2pix model is able to learn the mapping between the english character in japanese font, and the inputted characters to a great extent. The model is able to learn that the central character in the input image is the character that needs to be translated, while the 8 characters around the english character are there for style and font cues. The model is able to pick up and learn from the target character. The algorithm is able to capture the color and the curves of the target character successfully for some characters. But for some characters like Z the edges are not as sharp as intended, so some more experimentation is required there.
  </p>
  <p>The model is not perfect yet. We can see that it is failing  to understand the difference between some similar characters like “O” and “Q”, or “O” and “D”. The model is also not yet able to capture very complex styles like the shaded “B” yet.
  </p>
  <img src="img/visualization/failed.png" alt="Dataset Creation" height="400" width="800" style="padding-left: 20%;">

  <p>To see the progress of model learning. We plot some characters after just 5 epochs. We can see that the model seems to be learning like a child, trying to understand the shape of the characters, during the initial epochs.
  </p>
  <img src="img/visualization/epoch=5.png" alt="Dataset Creation" height="400" width="800" style="padding-left: 20%;">

  <h3 style="color: teal;">VISUALIZATION ANALYSIS</h3>
  <p>We trained the pix2pix model for Mapping from set A(including 8 jap and 1 english character) to set B(Target english character in Jap font). We trained the model for 100 epochs. The loss plots can be seen below.
  </p>
  <img src="img/visualization/loss.png" alt="Dataset Creation" height="600" width="900" style="padding-left: 12%;"><br>
  <ol>
    <li>G_GAN Loss: This loss represents the Generator's ability to create outputs that the Discriminator classifies as real. The G_GAN loss fluctuates over epochs, which is expected as the Generator learns. However, the general trend does not show a clear decrease, indicating that the Generator may struggle to improve in fooling the Discriminator consistently as the model is trained further.
    </li>
    <li>
      G_L1 Loss: The L1 loss typically measures the pixel-wise absolute difference between the generated images and the target images. The plot shows high variability but overall there is a downward trend. This suggests that the Generator is getting better at producing images that are closer to the target on average. Yet, the high variance implies there may be epochs where the model significantly diverges from the target, which could be a sign of instability in learning.
    </li>
    <li>
      D_real Loss: This metric reflects how well the Discriminator recognizes real images. The loss seems to have significant spikes but trends downward overall, which suggests that the Discriminator is becoming more proficient at identifying real images. <b>The spikes could indicate moments where the Discriminator's performance varies greatly, possibly due to changes in the Generator's outputs or due to the inherent variability of the training data.</b>
    </li>
    <li>
      D_fake Loss: This loss measures the Discriminator's ability to detect fake images from the Generator. The plot shows a downward trend, indicating that the Discriminator is improving at distinguishing fake images over time. <b>However, like the D_real loss, there is some volatility, suggesting that the model may have epochs where its performance degrades, which is common in adversarial training as the two networks are in a constant tug-of-war.</b>
    </li>
  </ol>



  <h3 style="color: teal;">EVALUATION</h3>
  <p>Our evaluation process for Japanese-to-English font style transfer employed a carefully curated dataset comprising pairs of Japanese text images and their corresponding English translations, encompassing diverse font styles.
    Our evaluation for Pix2Pix is based on the following:</p>
    <h4><u>EVALUATION METRICS</u></h4>
    <ul>
      <li><h4>Structural Similarity Index (SSIM):</h4> Measures the similarity in structural information between two images, assessing their visual resemblance.<br><br>
        Achieving an average score of 0.85, indicating the model's proficiency in preserving the visual style and essence of the original Japanese text.</li>

    <li><h4>Peak Signal-to-Noise Ratio (PSNR):</h4> Computed using the mean squared error between two images. PSNR quantifies the quality of an image by evaluating the level of noise or distortion, providing a higher score for images with lower distortion.
      <br><br>Achieving an average score of 25.7, highlighting the model's capability to maintain visual quality and fidelity in most cases.
    </ul>
    <pre>

      <b>Libraries involved</b>
      <code class="language-python" >
        import torch
        import torchvision.transforms as transforms
        from torch.utils.data import DataLoader
        from skimage import io
        from skimage.metrics import structural_similarity as ssim
        import numpy as np
      </code>
    </pre>
    <h4><u>MODEL PERFORMANCE</u></h4>
    <ul>
      <li>The scores, surpassing average expectations, underscore the model's robust performance in preserving both stylistic and content-related aspects of font style transfer.</li><br>
    <li>While commendable, the PSNR metric revealed subtle challenges in distinguishing certain characters (e.g., 'Q' and 'O'), resulting in minor visual artifacts.</li><br>
    <li>These findings provide valuable insights into the model's strengths and areas for improvement, offering a comprehensive assessment of its effectiveness in this nuanced task.</li>
    </li>
    </ul>

    <h3 style="color: teal;">NEXT STEPS</h3>
    For the next steps, we plan on doing the following to bring the project to completion:
    <ul>
    <li>We will train and generate results from another state of the art model, CycleGAN as mentinoned in the proposed methods.</li>
    <li>We will compare the results generated by Pix2Pix with CycleGAN</li><br>
    <li>We improve the results generated by the models by tackling the shortcomings using techniques such as Hyperparameter tuning.</li><br>
    </li>
    </ul>
  </section>

<section id="references">
<h2>References</h2>
<ol>
<li>[1] Isola, Phillip, et al. "Image-to-image translation with conditional adversarial networks." <i>Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</i></li>
<li>[2] Wang, Ting-Chun, et al. "High-resolution image synthesis and semantic manipulation with conditional gans." <i>Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.</i></li>
<li>[3] Zhu, Jun-Yan, et al. "Unpaired image-to-image translation using cycle-consistent adversarial networks." <i>Proceedings of the IEEE international conference on computer vision. 2017.</i></li>
</ol>
</section>

<section id="timeline">
<h2>Proposed Timeline</h2>
<img src="img/gantOverview.png" alt="High Level Gantt" height="300" width="900" style="padding-left: 15%;">
<br><br><button style="margin-left: 35%;"><a href="Project Timeline.xlsx">Click here for a Detailed view of the Gantt Chart</a></button>
</section>
<section id="contribution-table">
<h2>Contribution Table</h2>
<img src="img/contribution.png" alt="contribution-table" height="300" width="900" style="padding-left: 15%;">
</section>

<section id="presentation">
<h2>Video Presentation</h2>
<button style="margin-left: 45%;"><a href="https://www.youtube.com/watch?v=HjJcYWDH35g">Video Presentation Link</a></button><br>
</section>
<footer>
<p>Authors MSCS @Georgia Tech: Rachit Chadha, Khalid Shaikh, Mini Jain, Asmit Kumar Singh, Amrit Khera</p>
</footer>
</body>
</html>
